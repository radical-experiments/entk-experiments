__copyright__   = "Copyright 2017-2018, http://radical.rutgers.edu"
__author__      = "Vivek Balasubramanian <vivek.balasubramaniana@rutgers.edu>"
__license__     = "MIT"

import radical.utils as ru
from radical.entk.exceptions import *
from multiprocessing import Process, Event
from radical.entk import states, Pipeline, Task
from radical.entk.utils.init_transition import transition
import time
from time import sleep
import json
import threading
import pika
import traceback
import os
import uuid

class WFprocessor(object):

    """
    An WFProcessor (workflow processor) takes the responsibility of dispatching tasks from the various pipelines of the
    workflow according to their relative order to the TaskManager. All state updates are communicated to the AppManager.

    :Arguments:
        :workflow: COPY of the entire workflow existing in the AppManager 
        :pending_qs: number of queues to hold pending tasks
        :completed_qs: number of queues to hold completed tasks
        :mq_hostname: hostname where the RabbitMQ is live
    """


    def __init__(self, workflow, pending_queue, completed_queue, mq_hostname):

        self._uid           = ru.generate_id('radical.entk.wfprocessor')        
        self._logger        = ru.get_logger('radical.entk.wfprocessor')
        self._prof = ru.Profiler(name = self._uid + '-obj')

        self._prof.prof('create wfp obj', uid=self._uid)

        self._workflow      = workflow

        if not isinstance(pending_queue,list):
            raise TypeError(expected_type=list, actual_type=type(pending_queue))

        if not isinstance(completed_queue,list):
            raise TypeError(expected_type=list, actual_type=type(completed_queue))

        if not isinstance(mq_hostname,str):
            raise TypeError(expected_type=str, actual_type=type(mq_hostname))

        # Mqs queue names and channel
        self._pending_queue = pending_queue
        self._completed_queue = completed_queue
        self._mq_hostname = mq_hostname

        self._wfp_process = None       
        self._resubmit_failed = False       

        self._logger.info('Created WFProcessor object: %s'%self._uid)

        self._prof.prof('wfp obj created', uid=self._uid)
    

    # ------------------------------------------------------------------------------------------------------------------
    # Private Methods
    # ------------------------------------------------------------------------------------------------------------------

    def _wfp(self):

        """
        **Purpose**: This is the function executed in the wfp process. The function is used to simply create
        and spawn two threads: enqueue, dequeue. The enqueue thread pushes ready tasks to the queues in the pending_q slow
        list whereas the dequeue thread pulls completed tasks from the queues in the completed_q. This function is also
        responsible for the termination of these threads and hence blocking.
        """

        try:

            local_prof = ru.Profiler(name = self._uid + '-proc')

            local_prof.prof('wfp process started', uid=self._uid)

            self._logger.info('WFprocessor started')

            # Process should run till terminate condtion is encountered
            while (not self._wfp_terminate.is_set()):

                try:

                    # Start dequeue thread
                    if (not self._dequeue_thread) or (not self._dequeue_thread.is_alive()):

                        local_prof.prof('creating dequeue-thread', uid=self._uid)
                        self._dequeue_thread = threading.Thread(target=self._dequeue, args=(local_prof,), name='dequeue-thread')

                        self._logger.info('Starting dequeue-thread')
                        local_prof.prof('starting dequeue-thread', uid=self._uid)
                        self._dequeue_thread.start()

                    # Start enqueue thread
                    if (not self._enqueue_thread) or (not self._enqueue_thread.is_alive()):

                        local_prof.prof('creating enqueue-thread', uid=self._uid)
                        self._enqueue_thread = threading.Thread(target=self._enqueue, args=(local_prof,), name='enqueue-thread')

                        self._logger.info('Starting enqueue-thread')
                        local_prof.prof('starting enqueue-thread', uid=self._uid)
                        self._enqueue_thread.start()

                except Exception, ex:
                    self._logger.error('WFProcessor interrupted')
                    raise

            local_prof.prof('start termination', uid=self._uid)

            self._logger.info('Terminating enqueue-thread')
            self._enqueue_thread_terminate.set()
            self._enqueue_thread.join()
            self._logger.info('Terminating dequeue-thread')
            self._dequeue_thread_terminate.set()
            self._dequeue_thread.join()

            local_prof.prof('termination done', uid=self._uid)

            local_prof.prof('terminating wfp process', uid=self._uid)

            local_prof.close()

        except KeyboardInterrupt:

            self._logger.error('Execution interrupted by user (you probably hit Ctrl+C), '+
                                'trying to cancel wfprocessor process gracefully...')

            if self._enqueue_thread:

                if not self._enqueue_thread_terminate.is_set():
                    self._logger.info('Terminating enqueue-thread')
                    self._enqueue_thread_terminate.set()
                    self._enqueue_thread.join()

            if self._dequeue_thread:

                if not self._dequeue_thread_terminate.is_set():
                    self._logger.info('Terminating dequeue-thread')
                    self._dequeue_thread_terminate.set()
                    self._dequeue_thread.join()

            self._logger.info('WFprocessor process terminated')

            raise KeyboardInterrupt


        except Exception, ex:
            self._logger.error('Error in wfp process: %s. \n Closing enqueue, dequeue threads'%ex)

            if self._enqueue_thread:

                if not self._enqueue_thread_terminate.is_set():
                    self._logger.info('Terminating enqueue-thread')
                    self._enqueue_thread_terminate.set()
                    self._enqueue_thread.join()

            if self._dequeue_thread:

                if not self._dequeue_thread_terminate.is_set():
                    self._logger.info('Terminating dequeue-thread')
                    self._dequeue_thread_terminate.set()
                    self._dequeue_thread.join()

            self._logger.info('WFprocessor process terminated')

            print traceback.format_exc()
            raise Error(text=ex)           


    def _enqueue(self, local_prof):

        """
        **Purpose**: This is the function that is run in the enqueue thread. This function extracts Tasks from the 
        copy of workflow that exists in the WFprocessor object and pushes them to the queues in the pending_q list.
        Since this thread works on the copy of the workflow, every state update to the Task, Stage and Pipeline is
        communicated back to the AppManager (master process) via the 'sync_with_master' function that has dedicated
        queues to communicate with the master.

        Details: Termination condition of this thread is set by the wfp process.
        """

        try:


            local_prof.prof('enqueue-thread started', uid=self._uid)
            self._logger.info('enqueue-thread started')

            mq_connection = pika.BlockingConnection(pika.ConnectionParameters(host=self._mq_hostname))
            mq_channel = mq_connection.channel()

            while not self._enqueue_thread_terminate.is_set():

                for pipe in self._workflow:

                    with pipe._stage_lock:

                        if not pipe.completed:
      
                            # Test if the pipeline is already in the final state
                            if pipe.state in states.FINAL:
                                continue

                            elif pipe.state == states.INITIAL:
                                
                                # Set state of pipeline to SCHEDULING if it is in INITIAL
                                transition( obj=pipe, 
                                            obj_type = 'Pipeline', 
                                            new_state = states.SCHEDULING, 
                                            channel = mq_channel,
                                            queue = 'enq-to-sync',
                                            profiler=local_prof, 
                                            logger=self._logger)
    
                            if pipe.stages[pipe.current_stage-1].state in [states.INITIAL]:
    
                                try:

                                    # Starting scheduling of tasks of current stage, so set state of stage to 
                                    # SCHEDULING
                                    executable_stage = pipe.stages[pipe.current_stage-1]

                                    transition( obj=executable_stage, 
                                                obj_type = 'Stage', 
                                                new_state = states.SCHEDULING, 
                                                channel = mq_channel,
                                                queue = 'enq-to-sync',
                                                profiler=local_prof, 
                                                logger=self._logger)
                                    
                                    executable_tasks = executable_stage.tasks
    
                                    tasks_submitted=False
    
                                    for executable_task in executable_tasks:
    
                                        if executable_task.state == states.INITIAL:
                                            
                                            # Set state of Tasks in current Stage to SCHEDULING
                                            transition( obj=executable_task, 
                                                        obj_type = 'Task', 
                                                        new_state = states.SCHEDULING, 
                                                        channel = mq_channel,
                                                        queue = 'enq-to-sync',
                                                        profiler=local_prof, 
                                                        logger=self._logger)                                            
    
                                            task_as_dict = json.dumps(executable_task.to_dict())
    
                                            self._logger.debug('Publishing task %s to %s'
                                                                        %(executable_task.uid,
                                                                            self._pending_queue[0]))


                                            # Put the task on one of the pending_queues
                                            mq_channel.basic_publish(   exchange='',
                                                                        routing_key=self._pending_queue[0],
                                                                        body=task_as_dict
                                                                        #properties=pika.BasicProperties(
                                                                            # make message persistent
                                                                            #delivery_mode = 2)
                                                                    )

                                            # Set state of Tasks in current Stage to SCHEDULED
                                            transition( obj=executable_task, 
                                                obj_type = 'Task', 
                                                new_state = states.SCHEDULED, 
                                                channel = mq_channel,
                                                queue = 'enq-to-sync',
                                                profiler=local_prof, 
                                                logger=self._logger)
                                                
                                            tasks_submitted = True
                                            self._logger.debug('Task %s published to queue'% executable_task.uid)                                                                                                
                                                
                                    
                                    # All tasks of current stage scheduled, so set state of stage to
                                    # SCHEDULED
                                    transition( obj=executable_stage, 
                                                obj_type = 'Stage', 
                                                new_state = states.SCHEDULED, 
                                                channel = mq_channel,
                                                queue = 'enq-to-sync',
                                                profiler=local_prof, 
                                                logger=self._logger)

                                    print 'State transition done'

                                    if tasks_submitted:
                                        tasks_submitted = False

                                                                                
                                except Exception, ex:
    
                                    # If there is an error, explicitly switching the state of Stage to INITIAL
                                    self._logger.error('Error while updating stage '+
                                                        'state, rolling back. Error: %s'%ex)

                                    executable_stage = pipe.stages[pipe.current_stage-1]

                                    transition( obj=executable_stage, 
                                                obj_type = 'Stage', 
                                                new_state = states.INITIAL,
                                                channel = mq_channel,
                                                queue = 'enq-to-sync',
                                                profiler=local_prof, 
                                                logger=self._logger)
                                        
                                    raise

            self._logger.info('Enqueue thread terminated')                                  
            mq_connection.close()

            local_prof.prof('terminating enqueue-thread', uid=self._uid)
                                    
        except KeyboardInterrupt:

            self._logger.error('Execution interrupted by user (you probably hit Ctrl+C), '+
                                'trying to cancel enqueuer thread gracefully...')

            mq_connection.close()

            raise KeyboardInterrupt

        except Exception, ex:

            self._logger.error('Error in enqueue-thread: %s'%ex)
            print traceback.format_exc()

            raise Error(text=ex) 



    def _dequeue(self, local_prof):

        """
        **Purpose**: This is the function that is run in the dequeue thread. This function extracts Tasks from the 
        completed queus and updates the copy of workflow that exists in the WFprocessor object.
        Since this thread works on the copy of the workflow, every state update to the Task, Stage and Pipeline is
        communicated back to the AppManager (master process) via the 'sync_with_master' function that has dedicated
        queues to communicate with the master.

        Details: Termination condition of this thread is set by the wfp process.
        """

        try:

            local_prof.prof('dequeue-thread started', uid=self._uid)
            self._logger.info('Dequeue thread started')

            mq_connection = pika.BlockingConnection(pika.ConnectionParameters(host=self._mq_hostname))
            mq_channel = mq_connection.channel()

            while not self._dequeue_thread_terminate.is_set():

                try:

                    method_frame, header_frame, body = mq_channel.basic_get(queue=self._completed_queue[0])

                    if body:

                        # Get task from the message
                        completed_task = Task()
                        completed_task.from_dict(json.loads(body))
                        self._logger.info('Got finished task %s from queue'%(completed_task.uid))                       

                        transition( obj=completed_task, 
                                    obj_type = 'Task', 
                                    new_state = states.DEQUEUEING, 
                                    channel = mq_channel,
                                    queue = 'deq-to-sync',
                                    profiler=local_prof, 
                                    logger=self._logger)

                        # Traverse the entire workflow to find out the correct Task
                        for pipe in self._workflow:

                            with pipe._stage_lock:

                                if not pipe.completed:

                                    if completed_task._parent_pipeline == pipe.uid:

                                        self._logger.debug('Found parent pipeline: %s'%pipe.uid)
                                
                                        for stage in pipe.stages:

                                            if completed_task._parent_stage == stage.uid:
                                                self._logger.debug('Found parent stage: %s'%(stage.uid))

                                                transition( obj=completed_task, 
                                                            obj_type = 'Task', 
                                                            new_state = states.DEQUEUED, 
                                                            channel = mq_channel,
                                                            queue = 'deq-to-sync',
                                                            profiler=local_prof, 
                                                            logger=self._logger)

                                                if completed_task.exit_code:
                                                    completed_task.state = states.FAILED

                                                    transition( obj=completed_task, 
                                                                obj_type = 'Task', 
                                                                new_state = states.FAILED, 
                                                                channel = mq_channel,
                                                                queue = 'deq-to-sync',
                                                                profiler=local_prof, 
                                                                logger=self._logger)
                                                else:
                                                    completed_task.state = states.DONE

                                                    transition( obj=completed_task, 
                                                                obj_type = 'Task', 
                                                                new_state = states.DONE, 
                                                                channel = mq_channel,
                                                                queue = 'deq-to-sync',
                                                                profiler=local_prof, 
                                                                logger=self._logger)
                                                
                                                for task in stage.tasks:

                                                    if task.uid == completed_task.uid:
                                                        task.state = str(completed_task.state)

                                                        if task.state == states.DONE:

                                                            if stage._check_stage_complete():

                                                                transition( obj=stage, 
                                                                            obj_type = 'Stage', 
                                                                            new_state = states.DONE, 
                                                                            channel = mq_channel,
                                                                            queue = 'deq-to-sync',
                                                                            profiler=local_prof, 
                                                                            logger=self._logger)

                                                                try:
                                                                    pipe._increment_stage()

                                                                    if pipe.completed:
                                                                        
                                                                        transition( obj=pipe, 
                                                                                    obj_type = 'Pipeline', 
                                                                                    new_state = states.DONE, 
                                                                                    channel = mq_channel,
                                                                                    queue = 'deq-to-sync',
                                                                                    profiler=local_prof, 
                                                                                    logger=self._logger)
                                                                    
                                                                except:
                                                                    pipe._decrement_stage()

                                                        elif task.state == states.FAILED:

                                                            if self._resubmit_failed:

                                                                try:
                                                                    new_task = Task()
                                                                    new_task._replicate(completed_task)

                                                                    pipe.stages[pipe.current_stage-1].add_tasks(new_task)

                                                                except Exception, ex:
                                                                    self._logger.error("Resubmission of task %s failed, error: %s"%
                                                                                                    (completed_task.uid,ex))
                                                                    raise

                                                            else:

                                                                if stage._check_stage_complete():
                                                                    
                                                                    transition( obj=stage, 
                                                                                obj_type = 'Stage', 
                                                                                new_state = states.DONE, 
                                                                                channel = mq_channel,
                                                                                queue = 'deq-to-sync',
                                                                                profiler=local_prof, 
                                                                                logger=self._logger)

                                                                    try:
                                                                        pipe._increment_stage()

                                                                        if pipe.completed:
                                                                        
                                                                            transition( obj=pipe, 
                                                                                        obj_type = 'Pipeline', 
                                                                                        new_state = states.DONE,
                                                                                        channel = mq_channel, 
                                                                                        queue = 'deq-to-sync',
                                                                                        profiler=local_prof, 
                                                                                        logger=self._logger)
                                                                    
                                                                    except:
                                                                        pipe._decrement_stage()

                                                        else:

                                                            # Task is canceled
                                                            pass

                                                        # Found the task and processed it -- no more iterations needed
                                                        break

                                                # Found the stage and processed it -- no more iterations neeeded
                                                break

                                        # Found the pipeline and processed it -- no more iterations neeeded
                                        break

                        mq_channel.basic_ack(delivery_tag=method_frame.delivery_tag)

                except Exception, ex:
                    self._logger.error('Unable to receive message from completed queue: %s'%ex)
                    raise


            self._logger.info('Terminated dequeue thread')
            mq_connection.close()

            local_prof.prof('terminating dequeue-thread', uid=self._uid)

        except KeyboardInterrupt:

            self._logger.error('Execution interrupted by user (you probably hit Ctrl+C), '+
                            'trying to exit gracefully...')

            mq_connection.close()

            raise KeyboardInterrupt

        except Exception, ex:
            self._logger.error('Error in dequeue-thread: %s'%ex)
            print traceback.format_exc()

            mq_connection.close()

            raise Error(text=ex)



    # ------------------------------------------------------------------------------------------------------------------
    # Public Methods
    # ------------------------------------------------------------------------------------------------------------------

    def start_processor(self):

        """
        **Purpose**: Method to start the wfp process. The wfp function
        is not to be accessed directly. The function is started in a separate
        process using this method.
        """

        if not self._wfp_process:

            try:

                self._prof.prof('creating wfp process', uid=self._uid)
                self._wfp_process = Process(target=self._wfp, name='wfprocessor')

                self._enqueue_thread = None
                self._dequeue_thread = None
                self._enqueue_thread_terminate = threading.Event()
                self._dequeue_thread_terminate = threading.Event()

                self._wfp_terminate = Event()
                self._logger.info('Starting WFprocessor process')
                self._prof.prof('starting wfp process', uid=self._uid)
                self._wfp_process.start()                

                return True

            except Exception, ex:

                self._logger.error('WFprocessor not started')
                self.end_processor()
                raise       

        else:
            self._logger.warn('Wfp process already running, attempted to restart!')


    def end_processor(self):

        """
        **Purpose**: Method to terminate the wfp process. This method is 
        blocking as it waits for the wfp process to terminate (aka join).
        """

        try:
            
            self._logger.debug('Attempting to end WFprocessor... event: %s'%self._wfp_terminate.is_set())

            if self.check_alive():
                self._wfp_terminate.set()
                self._wfp_process.join()
                self._logger.debug('WFprocessor process terminated')
            else:
                self._logger.debug('WFprocessor process already terminated')

            self._prof.prof('wfp process terminated', uid=self._uid)

            self._prof.close()

        except Exception, ex:
            self._logger.error('Could not terminate wfprocessor process')
            raise

    def workflow_incomplete(self):

        """
        **Purpose**: Method to check if the workflow execution is incomplete.
        """
        
        try:
            for pipe in self._workflow:
                with pipe._stage_lock:
                    if pipe.completed:                    
                        pass
                    else:
                        return True
            return False

        except Exception, ex:
            self._logger.error('Could not check if workflow is incomplete, error:%s'%ex)
            raise


    def check_alive(self):

        """
        **Purpose**: Method to check if the wfp process is alive
        """

        return self._wfp_process.is_alive()